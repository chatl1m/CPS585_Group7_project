{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gcsfs\n",
        "from google.cloud import storage\n",
        "from google.colab import drive\n",
        "\n",
        "# Authenticate user and mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set project and bucket names\n",
        "project_id = 'cps585-finalproject-group7'\n",
        "bucket_name = 'cps585-finalbucket'\n",
        "\n",
        "# Create a storage client and get bucket object\n",
        "storage_client = storage.Client(project=project_id)\n",
        "bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "# Set file path and name\n",
        "file_path = '/content/drive/My Drive/project csv/imdb_movie_data1.csv'\n",
        "file_name = 'imdb_movie_data1.csv'\n",
        "\n",
        "# Create blob object and upload file to bucket\n",
        "blob = bucket.blob(file_name)\n",
        "blob.upload_from_filename(file_path)\n",
        "\n",
        "print(f\"{file_name} has been uploaded to {bucket_name} bucket in {project_id} project.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLN5oJdwOIha",
        "outputId": "030a8011-48ff-43a2-a13c-481340856d18"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gcsfs in /usr/local/lib/python3.9/dist-packages (2023.4.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.9/dist-packages (from gcsfs) (3.8.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from gcsfs) (2.27.1)\n",
            "Requirement already satisfied: fsspec==2023.4.0 in /usr/local/lib/python3.9/dist-packages (from gcsfs) (2023.4.0)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.9/dist-packages (from gcsfs) (2.17.3)\n",
            "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.9/dist-packages (from gcsfs) (4.4.2)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.9/dist-packages (from gcsfs) (2.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.9/dist-packages (from gcsfs) (1.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (2.0.12)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (6.0.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.3.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (4.0.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from google-auth>=1.2->gcsfs) (1.16.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth>=1.2->gcsfs) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth>=1.2->gcsfs) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth>=1.2->gcsfs) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib->gcsfs) (1.3.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.9/dist-packages (from google-cloud-storage->gcsfs) (2.3.2)\n",
            "Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from google-cloud-storage->gcsfs) (2.4.1)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.9/dist-packages (from google-cloud-storage->gcsfs) (2.11.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->gcsfs) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->gcsfs) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->gcsfs) (1.26.15)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.9/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs) (1.59.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.9/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs) (3.20.3)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.9/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage->gcsfs) (1.5.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.2.2)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "imdb_movie_data1.csv has been uploaded to cps585-finalbucket bucket in cps585-finalproject-group7 project.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code installs the gcsfs library and imports the required modules for Google Cloud Storage and Google Drive.Then, it mounts the user's Google Drive and sets the project_id and bucket_name variables.\n",
        "\n",
        "Next, it creates a Google Cloud Storage client using the storage.Client() method and retrieves the bucket object with the given bucket_name.After that, it sets the file_path and file_name variables, specifying the path and name of the file to be uploaded.\n",
        "\n",
        "Finally, it creates a blob object representing the file in the bucket, uploads the file from the specified file path to the blob, and prints a success message.So, this code uploads the imdb_movie_data1.csv file from the user's Google Drive to the cps585-finalbucket bucket in the cps585-finalproject-group7 project on Google Cloud Storage."
      ],
      "metadata": {
        "id": "1K9EWXyBtxeI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "dataset_id = \"my_dataset\"\n",
        "dataset_ref = client.dataset(dataset_id)\n",
        "\n",
        "client.delete_dataset(dataset_ref, delete_contents=True, not_found_ok=True)\n",
        "\n",
        "dataset = bigquery.Dataset(dataset_ref)\n",
        "dataset.location = \"US\"\n",
        "\n",
        "dataset = client.create_dataset(dataset, timeout=30)\n"
      ],
      "metadata": {
        "id": "QMWupb9tX9va"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code creates a new BigQuery dataset named \"my_dataset\" in the Google Cloud project specified by project_id variable. If the dataset already exists, it deletes it along with all its contents (delete_contents=True) and creates a new empty dataset with the same name. The dataset is created in the US region (dataset.location = \"US\") and the operation times out after 30 seconds (timeout=30). Finally, the dataset object is returned."
      ],
      "metadata": {
        "id": "jpTP3aagt7rb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kh7UGOunbl91",
        "outputId": "3d31cb4e-e18f-47ef-d3d0-f0235bff78bc"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['movie', 'year', 'certificate', 'genre.3', 'genre.3', 'genre.3',\n",
            "       'imdb_rating', 'metascore', 'time_minute', 'vote', 'gross_earning'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from google.cloud import bigquery\n",
        "from google.cloud import storage\n",
        "from google.cloud.exceptions import NotFound\n",
        "\n",
        "# Set project_id, bucket_name and file_name variables\n",
        "project_id = 'cps585-finalproject-group7'\n",
        "bucket_name = 'cps585-finalbucket'\n",
        "file_name = 'imdb_movie_data1.csv'\n",
        "dataset_id = 'imdb_movies'\n",
        "table_name = 'imdb_data'\n",
        "\n",
        "# Initialize BigQuery client\n",
        "bq_client = bigquery.Client(project=project_id)"
      ],
      "metadata": {
        "id": "sNO4iU2wk3Ys"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code imports the necessary libraries to work with Google Cloud's BigQuery and Storage services. Then, it sets the following variables:\n",
        "\n",
        "project_id: the ID of the Google Cloud project.\n",
        "bucket_name: the name of the Google Cloud Storage bucket where the data file is stored.\n",
        "file_name: the name of the CSV file containing the data.\n",
        "dataset_id: the name of the dataset that will be created in BigQuery to store the data.\n",
        "table_name: the name of the table that will be created in the dataset to store the data.\n",
        "Finally, it initializes a BigQuery client object bq_client with the given project ID."
      ],
      "metadata": {
        "id": "aWcBJsZhuQK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Storage client and get the bucket\n",
        "storage_client = storage.Client(project=project_id)\n",
        "bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "# Check if file exists in bucket\n",
        "try:\n",
        "    blob = bucket.blob(file_name)\n",
        "    blob.reload()\n",
        "except NotFound:\n",
        "    print('{} does not exist in {} bucket.'.format(file_name, bucket_name))\n",
        "else:\n",
        "    print('{} has been found in {} bucket.'.format(file_name, bucket_name))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLfSch3KoJBB",
        "outputId": "62956648-adfe-4a03-9e7a-f51ee9a1d168"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "imdb_movie_data1.csv has been found in cps585-finalbucket bucket.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is used to initialize the Google Cloud Storage client using the project ID provided in the project_id variable and get the specified bucket using the bucket_name variable.\n",
        "\n",
        "After that, the code tries to check whether the file_name (which in this case is imdb_movie_data1.csv) exists in the bucket using a try-except block.\n",
        "\n",
        "If the file exists, the code prints a message confirming that the file has been found in the specified bucket. If it does not exist, it prints a message saying that the file does not exist in the specified bucket.\n",
        "\n",
        "This check is useful before proceeding with any operations related to the file in the bucket, such as loading it into a BigQuery table."
      ],
      "metadata": {
        "id": "lpBGn8nGuae5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set dataset_ref\n",
        "dataset_ref = bq_client.dataset(dataset_id)\n",
        "\n",
        "# Create the dataset (if it doesn't exist)\n",
        "try:\n",
        "    bq_client.get_dataset(dataset_ref)\n",
        "except NotFound:\n",
        "    dataset = bigquery.Dataset(dataset_ref)\n",
        "    dataset = bq_client.create_dataset(dataset)\n",
        "    print('Created dataset {}.'.format(dataset_id))\n"
      ],
      "metadata": {
        "id": "hjGXxOS_oLdJ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code sets a reference to the BigQuery dataset specified by the dataset_id variable. It then tries to get the dataset using the get_dataset method of the BigQuery client. If the dataset is not found, it creates a new dataset using the create_dataset method of the BigQuery client and prints a message indicating that the dataset has been created."
      ],
      "metadata": {
        "id": "juugpNH2ulWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set table_id\n",
        "table_id = '{}.{}.{}'.format(project_id, dataset_id, table_name)\n",
        "\n",
        "# Set job_config for schema and delimiter\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    schema=[\n",
        "        bigquery.SchemaField('movie', 'STRING'),\n",
        "        bigquery.SchemaField('year', 'INTEGER'),\n",
        "        bigquery.SchemaField('certificate', 'STRING'),\n",
        "        bigquery.SchemaField('genre_1', 'STRING'),\n",
        "        bigquery.SchemaField('genre_2', 'STRING'),\n",
        "        bigquery.SchemaField('genre_3', 'STRING'),\n",
        "        bigquery.SchemaField('imdb_rating', 'FLOAT'),\n",
        "        bigquery.SchemaField('metascore', 'FLOAT'),\n",
        "        bigquery.SchemaField('time_minute', 'STRING'),\n",
        "        bigquery.SchemaField('vote', 'INTEGER'),\n",
        "        bigquery.SchemaField('gross_earning', 'STRING'),\n",
        "    ],\n",
        "    skip_leading_rows=1,\n",
        "    field_delimiter=','\n",
        ")\n"
      ],
      "metadata": {
        "id": "rXyfnTE9oN4L"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code sets the table_id variable which is a combination of the project_id, dataset_id and table_name. It also sets the job_config variable which includes the schema of the table to be created in BigQuery, as well as the delimiter and the number of rows to skip. The schema includes the field names and their corresponding data types, which is defined based on the columns of the CSV file that we are importing. The skip_leading_rows parameter specifies that the first row of the CSV file contains headers and should be skipped, while the field_delimiter parameter specifies that the fields are separated by commas."
      ],
      "metadata": {
        "id": "IMPvIyYuusZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the CSV file\n",
        "file_path = '/content/drive/My Drive/project csv/imdb_movie_data1.csv'\n",
        "\n",
        "# Replace \"-\" with \"0\" for metascore column in the CSV file\n",
        "with open(file_path, 'r') as csv_file:\n",
        "    reader = csv.reader(csv_file)\n",
        "    rows = []\n",
        "    for row in reader:\n",
        "        row[7] = row[7].replace('-', '0')\n",
        "        rows.append(row)\n",
        "\n",
        "# Overwrite the CSV file with the modified data\n",
        "with open(file_path, 'w', newline='') as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    for row in rows:\n",
        "        writer.writerow(row)\n",
        "\n",
        "print('Done replacing \"-\" with \"0\" for metascore column in the CSV file')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTrra1GNqNiR",
        "outputId": "21b94f90-5590-4887-ba37-49b6139c7349"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done replacing \"-\" with \"0\" for metascore column in the CSV file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code replaces all occurrences of \"-\" with \"0\" in the metascore column of the CSV file located at the specified file path.\n",
        "\n",
        "First, it opens the file using the open() function and reads its contents using the csv.reader() function. It then loops through each row in the file and replaces any \"-\" value in the 7th (or 8th) column (i.e., the metascore column) with \"0\". The modified rows are then appended to a new list called rows.\n",
        "\n",
        "After modifying the data, the code then overwrites the original CSV file using the csv.writer() function to write each row to the file. Finally, it prints a message indicating that the replacement is done."
      ],
      "metadata": {
        "id": "9eCOp22Yu0AH"
      }
    }
  ]
}